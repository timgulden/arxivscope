#!/usr/bin/env python3
"""
Aipickle Ingester using the Shared Ingestion Framework
Processes aipickle data using the reliable patterns from the shared framework.
"""

import sys
import argparse
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Optional, Iterator, Tuple
from datetime import datetime

import re # Added missing import for re
import json

# Import the shared framework
import sys
sys.path.append('..')  # Add parent directory to path to access shared_ingestion_framework
from shared_ingestion_framework import process_file_unified, get_default_config, PaperRecord, MetadataRecord

def process_aipickle_pickle_file(file_path: Path) -> Iterator[Dict[str, Any]]:
    """
    Pure function: Process aipickle pickle file and yield individual records.
    This replaces the shared framework's JSON processing for aipickle's specific format.
    """
    try:
        df = pd.read_pickle(file_path)
        for _, row in df.iterrows():
            # Convert pandas Series to dict
            record = row.to_dict()
            yield record
    except Exception as e:
        print(f"Error processing aipickle file: {e}")
        raise

def transform_aipickle_record(record: Dict[str, Any]) -> Optional[PaperRecord]:
    """
    Pure function: Transform an aipickle record to PaperRecord format.
    
    Args:
        record: Raw aipickle record from pickle file
        
    Returns:
        Transformed PaperRecord or None if invalid
    """
    try:
        # Extract required fields
        title = record.get('Title', '').strip()
        if not title:
            return None
            
        source_id = record.get('Paper ID', '').strip()
        if not source_id:
            return None
            
        # Extract optional fields
        abstract = record.get('Summary', '').strip()
        authors_str = record.get('Authors', '')
        submitted_date = record.get('Submitted Date', '').strip()
        
        # Parse authors
        if authors_str:
            # Split by comma and clean up
            authors = [author.strip() for author in authors_str.split(',') if author.strip()]
            authors_tuple = tuple(authors)
        else:
            authors_tuple = ()
        
        # Parse submission date
        parsed_date = parse_submission_date(submitted_date)
        
        return PaperRecord(
            source='aipickle',
            source_id=source_id,
            title=title,
            abstract=abstract,
            authors=authors_tuple,
            primary_date=parsed_date
        )
        
    except Exception as e:
        print(f"Error transforming aipickle record: {e}")
        return None

def parse_submission_date(date_str: str) -> Optional[str]:
    """
    Pure function: Parse submission date and extract year.
    
    Args:
        date_str: Raw date string from aipickle
        
    Returns:
        ISO date string (YYYY-01-01) or None
    """
    if not date_str:
        return None
    
    try:
        # Try to parse the date
        parsed_date = datetime.strptime(date_str, '%Y-%m-%d')
        return parsed_date.strftime('%Y-%m-%d')
    except (ValueError, TypeError):
        # If parsing fails, try to extract year
        try:
            year_match = re.search(r'(\d{4})', date_str)
            if year_match:
                year = int(year_match.group(1))
                return f"{year}-01-01"  # Use January 1st as default
        except:
            pass
    
    return None

def extract_aipickle_metadata(record: Dict[str, Any], paper_id: str) -> Dict[str, str]:
    """
    Pure function: Extract metadata from aipickle record for storage in source-specific table.
    
    Args:
        record: Raw aipickle record
        paper_id: The paper ID (not used in this case)
        
    Returns:
        Metadata dictionary
    """
    metadata = {
        'doctrove_paper_id': paper_id,  # Will be set by the framework
        'link': record.get('Link', ''),
        'updated': record.get('Updated', ''),
        'author_affiliations': record.get('Author Affiliations', ''),
        'links': record.get('Links', ''),
        'categories': record.get('Categories', ''),
        'primary_category': record.get('Primary Category', ''),
        'comment': record.get('Comment', ''),
        'journal_ref': record.get('Journal Ref', ''),
        'doi': record.get('DOI', ''),
        'category': record.get('category', ''),
        'country_of_origin': record.get('country of origin', ''),
        'country': record.get('country', ''),
        'country2': record.get('Country2', '')
    }
    
    # Note: title_embedding field is obsolete and has been removed
    # All embeddings are now generated by the enrichment service
    
    # Convert dictionaries to JSON strings for database storage
    processed_metadata = {}
    for k, v in metadata.items():
        if v and str(v).strip():
            if isinstance(v, (dict, list)):
                processed_metadata[k] = json.dumps(v)
            else:
                processed_metadata[k] = str(v)
    
    return processed_metadata

def get_aipickle_metadata_fields() -> List[str]:
    """Pure function: Get the list of metadata fields for aipickle data."""
    return [
        'doctrove_paper_id',
        'link',
        'updated',
        'author_affiliations',
        'links',
        'categories',
        'primary_category',
        'comment',
        'journal_ref',
        'doi',
        'category',
        'country_of_origin',
        'country',
        'country2'
    ]

def filter_aipickle_records(records: Iterator[Dict[str, Any]]) -> Iterator[Dict[str, Any]]:
    """
    Pure function: Filter aipickle records using validation logic.
    This applies aipickle-specific filtering before transformation.
    """
    def is_valid_record(record: Dict[str, Any]) -> bool:
        # Check for required fields
        title = record.get('Title', '').strip()
        paper_id = record.get('Paper ID', '').strip()
        return bool(title and paper_id)
    
    return filter(is_valid_record, records)

def process_aipickle_file_unified(file_path: Path, config_provider=None, limit: Optional[int] = None):
    """
    Main entry point for aipickle processing using the shared framework.
    
    This function customizes the shared framework for aipickle's specific needs:
    - Uses pickle file processing
    - Applies aipickle-specific filtering
    - Uses aipickle transformation logic
    - Uses aipickle metadata extraction
    """
    from shared_ingestion_framework import (
        process_file_unified, 
        limit_records,
        transform_records_to_papers,
        filter_valid_papers,
        ensure_metadata_table_exists,
        insert_paper_with_metadata,
        extract_metadata_from_record,
        ProcessingResult
    )
    
    if config_provider is None:
        config_provider = get_default_config
    
    # Import connection factory
    from shared_ingestion_framework import create_connection_factory
    connection_factory = create_connection_factory(config_provider)
    
    # Ensure metadata table exists
    ensure_metadata_table_exists(connection_factory, 'aipickle', get_aipickle_metadata_fields())
    
    # Pure data processing pipeline (aipickle-specific)
    records = process_aipickle_pickle_file(file_path)
    records = limit_records(records, limit)
    records = filter_aipickle_records(records)  # Apply aipickle filtering
    
    # Convert to list for processing and keep track of original records
    records_list = list(records)
    papers = transform_records_to_papers(iter(records_list), transform_aipickle_record)
    papers = filter_valid_papers(papers)  # Apply shared framework validation
    
    # Convert to list for processing
    papers_list = list(papers)
    total_processed = len(papers_list)
    
    # Process papers (impure operations)
    inserted_count = 0
    errors = []
    
    for i, paper in enumerate(papers_list):
        try:
            # Extract metadata (pure) - use the original record data
            original_record = records_list[i] if i < len(records_list) else {}
            metadata = extract_metadata_from_record(original_record, paper.source_id, extract_aipickle_metadata)
            
            # Insert paper (impure)
            if insert_paper_with_metadata(connection_factory, paper, metadata, 'aipickle'):
                inserted_count += 1
                
                if inserted_count % 100 == 0:
                    print(f"Inserted {inserted_count} papers (processed {total_processed})")
        except Exception as e:
            errors.append(f"Error processing paper {paper.source_id}: {e}")
            continue
    
    print(f"Successfully inserted {inserted_count} papers from {file_path}")
    return ProcessingResult(inserted_count=inserted_count, total_processed=total_processed, errors=errors)

def main():
    """Main entry point for aipickle ingestion."""
    parser = argparse.ArgumentParser(description='Ingest aipickle data using unified framework')
    parser.add_argument('file_path', help='Path to aipickle pickle file')
    parser.add_argument('--limit', type=int, help='Limit number of records to process (for testing)')
    
    args = parser.parse_args()
    
    file_path = Path(args.file_path)
    if not file_path.exists():
        print(f"File not found: {file_path}")
        sys.exit(1)
    
    print(f"Processing aipickle data from: {file_path}")
    
    # Process the file using the unified framework
    try:
        result = process_aipickle_file_unified(
            file_path=file_path,
            config_provider=get_default_config,
            limit=args.limit
        )
        
        print(f"✅ Successfully processed aipickle data:")
        print(f"   - Papers inserted: {result.inserted_count}")
        print(f"   - Total processed: {result.total_processed}")
        if result.errors:
            print(f"   - Errors: {len(result.errors)}")
            for error in result.errors[:5]:  # Show first 5 errors
                print(f"     - {error}")
        
    except Exception as e:
        print(f"❌ Error processing aipickle data: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 