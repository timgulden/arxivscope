#!/usr/bin/env python3
"""
UMAP Rebuild Script for Focused Dataset (arXiv + RAND)

After removing OpenAlex records, we need to rebuild the UMAP model on the 
remaining arxiv, randpub, and extpub papers.

This script:
1. Samples papers with 1D embeddings from the focused dataset
2. Trains a fresh UMAP model on these papers
3. Saves the new model
4. Clears all 2D embeddings (they'll be regenerated by queue workers)

Usage:
    python rebuild_umap_for_focused_dataset.py --sample-size 50000
"""

import sys
import os
import argparse
import logging
import pickle
import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor
import umap.umap_ as umap

# Add parent directory for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'doctrove-api'))
from config import DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# UMAP configuration optimized for focused dataset
UMAP_CONFIG = {
    'n_components': 2,
    'n_neighbors': 15,
    'min_dist': 0.1,
    'metric': 'cosine',
    'random_state': 42
}

# Use relative path from script location, or fallback to models/ directory
import os
_script_dir = os.path.dirname(os.path.abspath(__file__))
_model_dir = os.path.join(_script_dir, '..', 'models')
MODEL_PATH = os.getenv('UMAP_MODEL_PATH', os.path.join(_model_dir, 'umap_model.pkl'))
OLD_MODEL_BACKUP = os.path.join(_model_dir, 'umap_model_old_with_openalex.pkl')


def get_connection():
    """Create database connection."""
    return psycopg2.connect(
        host=DB_HOST,
        port=DB_PORT,
        dbname=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD
    )


def backup_old_model():
    """Backup the old UMAP model before replacing it."""
    if os.path.exists(MODEL_PATH):
        logger.info(f"Backing up old UMAP model to: {OLD_MODEL_BACKUP}")
        os.rename(MODEL_PATH, OLD_MODEL_BACKUP)
        logger.info(f"Old model backed up successfully")
    else:
        logger.info("No existing model to backup")


def sample_embeddings(sample_size: int) -> tuple:
    """
    Sample papers with 1D embeddings from the focused dataset.
    
    Returns:
        tuple: (embeddings array, paper_ids list)
    """
    logger.info(f"Sampling {sample_size:,} papers with 1D embeddings...")
    
    conn = get_connection()
    cursor = conn.cursor(cursor_factory=RealDictCursor)
    
    try:
        # Count available papers with embeddings
        cursor.execute("""
            SELECT COUNT(*) as total
            FROM doctrove_papers
            WHERE doctrove_embedding IS NOT NULL
            AND doctrove_source IN ('arxiv', 'randpub', 'extpub')
        """)
        total = cursor.fetchone()['total']
        logger.info(f"Total papers with 1D embeddings: {total:,}")
        
        if total < sample_size:
            logger.warning(f"Only {total:,} papers available, using all of them")
            sample_size = total
        
        # Sample papers randomly
        cursor.execute("""
            SELECT doctrove_paper_id, doctrove_embedding
            FROM doctrove_papers
            WHERE doctrove_embedding IS NOT NULL
            AND doctrove_source IN ('arxiv', 'randpub', 'extpub')
            ORDER BY RANDOM()
            LIMIT %s
        """, (sample_size,))
        
        papers = cursor.fetchall()
        logger.info(f"Fetched {len(papers):,} papers")
        
        # Extract embeddings as numpy array
        # Note: pgvector can return embeddings as strings or arrays
        def parse_embedding(embedding_data):
            """Parse embedding from pgvector (can be string or array)."""
            if isinstance(embedding_data, str):
                # Parse string format: '[1.0, 2.0, 3.0]'
                embedding_data = embedding_data.strip('[]').split(',')
                return np.array([float(x.strip()) for x in embedding_data], dtype=np.float32)
            else:
                # Already an array-like object
                return np.array(embedding_data, dtype=np.float32)
        
        embeddings = np.array([
            parse_embedding(paper['doctrove_embedding']) 
            for paper in papers
        ])
        
        paper_ids = [paper['doctrove_paper_id'] for paper in papers]
        
        logger.info(f"Embedding shape: {embeddings.shape}")
        return embeddings, paper_ids
        
    finally:
        cursor.close()
        conn.close()


def train_umap_model(embeddings: np.ndarray) -> umap.UMAP:
    """
    Train UMAP model on the embeddings.
    
    Args:
        embeddings: Numpy array of shape (n_samples, embedding_dim)
        
    Returns:
        Trained UMAP model
    """
    logger.info("Training UMAP model...")
    logger.info(f"UMAP config: {UMAP_CONFIG}")
    
    model = umap.UMAP(**UMAP_CONFIG)
    
    # Fit the model
    logger.info("Fitting UMAP (this may take several minutes)...")
    model.fit(embeddings)
    
    logger.info("UMAP training complete!")
    return model


def save_umap_model(model: umap.UMAP):
    """Save the trained UMAP model to disk."""
    logger.info(f"Saving UMAP model to: {MODEL_PATH}")
    
    with open(MODEL_PATH, 'wb') as f:
        pickle.dump(model, f)
    
    # Check file size
    size_mb = os.path.getsize(MODEL_PATH) / (1024 * 1024)
    logger.info(f"Model saved successfully ({size_mb:.1f} MB)")


def clear_2d_embeddings():
    """Clear all existing 2D embeddings so they can be regenerated."""
    logger.info("Clearing existing 2D embeddings...")
    
    conn = get_connection()
    cursor = conn.cursor()
    
    try:
        # Check how many need to be cleared
        cursor.execute("""
            SELECT COUNT(*) as total
            FROM doctrove_papers
            WHERE doctrove_embedding_2d IS NOT NULL
        """)
        total = cursor.fetchone()[0]
        logger.info(f"Found {total:,} papers with 2D embeddings to clear")
        
        # Clear them
        cursor.execute("""
            UPDATE doctrove_papers
            SET doctrove_embedding_2d = NULL,
                embedding_2d_updated_at = NULL
            WHERE doctrove_embedding_2d IS NOT NULL
        """)
        
        conn.commit()
        logger.info(f"Cleared 2D embeddings for {total:,} papers")
        
        # Queue them for regeneration (if enrichment_queue table exists)
        try:
            cursor.execute("""
                INSERT INTO enrichment_queue (paper_id, enrichment_type, priority, status)
                SELECT doctrove_paper_id, 'embedding_2d', 1, 'pending'
                FROM doctrove_papers
                WHERE doctrove_embedding IS NOT NULL
                AND doctrove_embedding_2d IS NULL
                ON CONFLICT DO NOTHING
            """)
            conn.commit()
            queued = cursor.rowcount
            logger.info(f"Queued {queued:,} papers for 2D embedding regeneration")
        except Exception as e:
            logger.warning(f"Could not queue papers (enrichment system may need manual restart): {e}")
        
    finally:
        cursor.close()
        conn.close()


def main():
    parser = argparse.ArgumentParser(
        description='Rebuild UMAP model for focused dataset (arXiv + RAND)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Standard rebuild with 50K sample
  python rebuild_umap_for_focused_dataset.py --sample-size 50000
  
  # Larger sample for better quality
  python rebuild_umap_for_focused_dataset.py --sample-size 100000
  
  # Use all available papers (may be slow)
  python rebuild_umap_for_focused_dataset.py --sample-size 9999999
        """
    )
    
    parser.add_argument(
        '--sample-size',
        type=int,
        default=50000,
        help='Number of papers to sample for UMAP training (default: 50000)'
    )
    
    parser.add_argument(
        '--skip-backup',
        action='store_true',
        help='Skip backing up the old model'
    )
    
    parser.add_argument(
        '--no-clear',
        action='store_true',
        help='Do not clear existing 2D embeddings (for testing)'
    )
    
    args = parser.parse_args()
    
    logger.info("=" * 80)
    logger.info("UMAP Model Rebuild for Focused Dataset")
    logger.info("=" * 80)
    logger.info(f"Sample size: {args.sample_size:,}")
    logger.info(f"Sources: arxiv, randpub, extpub")
    logger.info("")
    
    try:
        # Step 1: Backup old model
        if not args.skip_backup:
            backup_old_model()
        
        # Step 2: Sample embeddings
        embeddings, paper_ids = sample_embeddings(args.sample_size)
        
        # Step 3: Train UMAP model
        model = train_umap_model(embeddings)
        
        # Step 4: Save model
        save_umap_model(model)
        
        # Step 5: Clear existing 2D embeddings
        if not args.no_clear:
            clear_2d_embeddings()
        
        logger.info("")
        logger.info("=" * 80)
        logger.info("âœ… UMAP Rebuild Complete!")
        logger.info("=" * 80)
        logger.info(f"New model trained on {len(embeddings):,} papers")
        logger.info(f"Old model backed up to: {OLD_MODEL_BACKUP}")
        logger.info(f"New model saved to: {MODEL_PATH}")
        logger.info("")
        logger.info("Next steps:")
        logger.info("1. Start the 2D embedding worker:")
        logger.info(f"   cd {os.path.dirname(os.path.abspath(__file__))}")
        logger.info("   screen -dmS embedding_2d python queue_2d_worker.py")
        logger.info("")
        logger.info("2. Monitor progress:")
        logger.info("   screen -r embedding_2d")
        logger.info("")
        
    except Exception as e:
        logger.error(f"Error during UMAP rebuild: {e}", exc_info=True)
        sys.exit(1)


if __name__ == '__main__':
    main()

